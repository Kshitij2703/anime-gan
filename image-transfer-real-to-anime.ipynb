{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":627400,"sourceType":"datasetVersion","datasetId":308470}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport os\nfrom PIL import Image\nimport numpy as np\nimport itertools\nimport random\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-30T06:23:16.943006Z","iopub.execute_input":"2024-10-30T06:23:16.944091Z","iopub.status.idle":"2024-10-30T06:23:16.949515Z","shell.execute_reply.started":"2024-10-30T06:23:16.944049Z","shell.execute_reply":"2024-10-30T06:23:16.948492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AnimeDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = os.listdir(root_dir)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root_dir, self.images[idx])\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:23:20.220730Z","iopub.execute_input":"2024-10-30T06:23:20.221462Z","iopub.status.idle":"2024-10-30T06:23:20.227831Z","shell.execute_reply.started":"2024-10-30T06:23:20.221422Z","shell.execute_reply":"2024-10-30T06:23:20.226945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n        \n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.InstanceNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.InstanceNorm2d(in_channels)\n        )\n        \n    def forward(self, x):\n        return x + self.block(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:23:22.229799Z","iopub.execute_input":"2024-10-30T06:23:22.230214Z","iopub.status.idle":"2024-10-30T06:23:22.237122Z","shell.execute_reply.started":"2024-10-30T06:23:22.230169Z","shell.execute_reply":"2024-10-30T06:23:22.236170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        # Initial convolution block\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Downsampling blocks\n        self.down_sampling = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Residual blocks\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(256) for _ in range(6)]\n        )\n        \n        # Upsampling blocks\n        self.up_sampling = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Output layer\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3),\n            nn.Tanh()\n        )\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.down_sampling(x)\n        x = self.residual_blocks(x)\n        x = self.up_sampling(x)\n        x = self.conv2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:23:23.979611Z","iopub.execute_input":"2024-10-30T06:23:23.980358Z","iopub.status.idle":"2024-10-30T06:23:23.991819Z","shell.execute_reply.started":"2024-10-30T06:23:23.980311Z","shell.execute_reply":"2024-10-30T06:23:23.990840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n            nn.InstanceNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n        )\n        \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:23:27.197480Z","iopub.execute_input":"2024-10-30T06:23:27.197848Z","iopub.status.idle":"2024-10-30T06:23:27.206354Z","shell.execute_reply.started":"2024-10-30T06:23:27.197814Z","shell.execute_reply":"2024-10-30T06:23:27.205411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(images, title=''):\n    \"\"\"Display a batch of images\"\"\"\n    plt.figure(figsize=(10, 10))\n    plt.title(title)\n    plt.imshow(np.transpose(utils.make_grid(images[:16], nrow=4, padding=2, normalize=True).cpu(), (1, 2, 0)))\n    plt.axis('off')\n    plt.show()\n\ndef save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n    \"\"\"Save checkpoint\"\"\"\n    torch.save(state, filename)\n    print(f\"Checkpoint saved: {filename}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:23:29.475227Z","iopub.execute_input":"2024-10-30T06:23:29.475665Z","iopub.status.idle":"2024-10-30T06:23:29.482354Z","shell.execute_reply.started":"2024-10-30T06:23:29.475626Z","shell.execute_reply":"2024-10-30T06:23:29.481317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.batch_size = 4  \n        \n        # Learning parameters\n        self.lr = 0.0002\n        self.beta1 = 0.5\n        self.beta2 = 0.999\n        \n        self.num_epochs = 200  \n        \n        self.lambda_cycle = 8.0  \n        self.lambda_identity = 3.0  \n        \n        self.image_size = 128  \n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:54:31.187819Z","iopub.execute_input":"2024-10-30T06:54:31.188530Z","iopub.status.idle":"2024-10-30T06:54:31.194458Z","shell.execute_reply.started":"2024-10-30T06:54:31.188487Z","shell.execute_reply":"2024-10-30T06:54:31.193281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(anime_dir, real_dir, config):\n    transform = transforms.Compose([\n        transforms.Resize(config.image_size),\n        transforms.RandomCrop(config.image_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    # Load datasets\n    anime_dataset = AnimeDataset(anime_dir, transform=transform)\n    real_dataset = AnimeDataset(real_dir, transform=transform)\n    \n    # Use 40% of anime images for training\n    anime_size = int(len(anime_dataset) * 0.4)\n    indices = random.sample(range(len(anime_dataset)), anime_size)\n    anime_dataset = torch.utils.data.Subset(anime_dataset, indices)\n    \n    anime_loader = DataLoader(anime_dataset, batch_size=config.batch_size, shuffle=True)\n    real_loader = DataLoader(real_dataset, batch_size=config.batch_size, shuffle=True)\n    \n    return anime_loader, real_loader","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:54:33.025131Z","iopub.execute_input":"2024-10-30T06:54:33.025873Z","iopub.status.idle":"2024-10-30T06:54:33.033511Z","shell.execute_reply.started":"2024-10-30T06:54:33.025833Z","shell.execute_reply":"2024-10-30T06:54:33.032518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_style_transfer(anime_dir, real_dir):\n    config = Config()\n    \n    # Load data\n    anime_loader, real_loader = load_data(anime_dir, real_dir, config)\n    \n    # Initialize networks\n    G_real2anime = Generator().to(config.device)\n    G_anime2real = Generator().to(config.device)\n    D_anime = Discriminator().to(config.device)\n    D_real = Discriminator().to(config.device)\n    \n    # Loss functions\n    criterion_GAN = nn.MSELoss()\n    criterion_cycle = nn.L1Loss()\n    criterion_identity = nn.L1Loss()\n    \n    # Optimizers\n    optimizer_G = optim.Adam(\n        itertools.chain(G_real2anime.parameters(), G_anime2real.parameters()),\n        lr=config.lr, betas=(config.beta1, config.beta2)\n    )\n    optimizer_D_A = optim.Adam(D_anime.parameters(), lr=config.lr, betas=(config.beta1, config.beta2))\n    optimizer_D_B = optim.Adam(D_real.parameters(), lr=config.lr, betas=(config.beta1, config.beta2))\n    \n    max_batches_per_epoch = 100  # i have reduced batch size due to  memory issues\n\n    # Training loop\n    for epoch in range(config.num_epochs):\n        for i, (real_imgs, anime_imgs) in enumerate(zip(real_loader, anime_loader)):\n            if i >= max_batches_per_epoch:\n                break  # End epoch early if max_batches_per_epoch is reached\n\n            real_imgs = real_imgs.to(config.device)\n            anime_imgs = anime_imgs.to(config.device)\n            \n            # Generate fake images\n            fake_anime = G_real2anime(real_imgs)\n            fake_real = G_anime2real(anime_imgs)\n            \n            # Train Generators\n            optimizer_G.zero_grad()\n            \n            # Identity loss\n            loss_id_anime = criterion_identity(G_real2anime(anime_imgs), anime_imgs)\n            loss_id_real = criterion_identity(G_anime2real(real_imgs), real_imgs)\n            \n            # GAN loss\n            loss_GAN_real2anime = criterion_GAN(D_anime(fake_anime), torch.ones_like(D_anime(fake_anime)))\n            loss_GAN_anime2real = criterion_GAN(D_real(fake_real), torch.ones_like(D_real(fake_real)))\n            \n            # Cycle loss\n            recovered_real = G_anime2real(fake_anime)\n            recovered_anime = G_real2anime(fake_real)\n            loss_cycle_real = criterion_cycle(recovered_real, real_imgs)\n            loss_cycle_anime = criterion_cycle(recovered_anime, anime_imgs)\n            \n            # Total generator loss\n            loss_G = (loss_GAN_real2anime + loss_GAN_anime2real +\n                      (loss_cycle_real + loss_cycle_anime) * config.lambda_cycle +\n                      (loss_id_anime + loss_id_real) * config.lambda_identity)\n            \n            loss_G.backward()\n            optimizer_G.step()\n            \n            # Train Discriminator A (Anime)\n            optimizer_D_A.zero_grad()\n            loss_real_A = criterion_GAN(D_anime(anime_imgs), torch.ones_like(D_anime(anime_imgs)))\n            loss_fake_A = criterion_GAN(D_anime(fake_anime.detach()), torch.zeros_like(D_anime(fake_anime)))\n            loss_D_A = (loss_real_A + loss_fake_A) * 0.5\n            loss_D_A.backward()\n            optimizer_D_A.step()\n            \n            # Train Discriminator B (Real)\n            optimizer_D_B.zero_grad()\n            loss_real_B = criterion_GAN(D_real(real_imgs), torch.ones_like(D_real(real_imgs)))\n            loss_fake_B = criterion_GAN(D_real(fake_real.detach()), torch.zeros_like(D_real(fake_real)))\n            loss_D_B = (loss_real_B + loss_fake_B) * 0.5\n            loss_D_B.backward()\n            optimizer_D_B.step()\n            \n            if i % 10 == 0:  # Display update every 10 batches instead of every 100\n                print(f\"Epoch [{epoch}/{config.num_epochs}] Batch [{i}/{max_batches_per_epoch}] \"\n                      f\"Loss_D: {(loss_D_A + loss_D_B).item():.4f} \"\n                      f\"Loss_G: {loss_G.item():.4f} \"\n                      f\"Loss_cycle: {(loss_cycle_real + loss_cycle_anime).item():.4f}\")\n                \n                # Display current results\n                show_images(torch.cat([real_imgs, fake_anime, recovered_real], 0), \n                          f'Epoch {epoch} - Real → Anime → Recovered Real')\n        \n        # Save checkpoint\n        if (epoch + 1) % 10 == 0:\n            checkpoint = {\n                'epoch': epoch,\n                'G_real2anime': G_real2anime.state_dict(),\n                'G_anime2real': G_anime2real.state_dict(),\n                'D_anime': D_anime.state_dict(),\n                'D_real': D_real.state_dict(),\n                'optimizer_G': optimizer_G.state_dict(),\n                'optimizer_D_A': optimizer_D_A.state_dict(),\n                'optimizer_D_B': optimizer_D_B.state_dict()\n            }\n            save_checkpoint(checkpoint, f'checkpoint_epoch_{epoch+1}.pth.tar')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:54:35.691685Z","iopub.execute_input":"2024-10-30T06:54:35.692084Z","iopub.status.idle":"2024-10-30T06:54:35.712662Z","shell.execute_reply.started":"2024-10-30T06:54:35.692045Z","shell.execute_reply":"2024-10-30T06:54:35.711487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_anime(image_path, generator_path):\n    config = Config()\n    \n    # Load and preprocess the image\n    transform = transforms.Compose([\n        transforms.Resize(config.image_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(config.device)\n    \n    # Load the generator\n    generator = Generator().to(config.device)\n    checkpoint = torch.load(generator_path)\n    generator.load_state_dict(checkpoint['G_real2anime'])\n    generator.eval()\n    \n    # Convert image\n    with torch.no_grad():\n        anime_image = generator(image)\n        \n    # Post-process and save\n    anime_image = (anime_image * 0.5 + 0.5).clamp(0, 1)\n    anime_image = anime_image.squeeze().cpu().numpy()\n    anime_image = np.transpose(anime_image, (1, 2, 0))\n    anime_image = (anime_image * 255).astype(np.uint8)\n    \n    return Image.fromarray(anime_image)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:54:41.953099Z","iopub.execute_input":"2024-10-30T06:54:41.953516Z","iopub.status.idle":"2024-10-30T06:54:41.962261Z","shell.execute_reply.started":"2024-10-30T06:54:41.953478Z","shell.execute_reply":"2024-10-30T06:54:41.961292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anime_dir = '/kaggle/input/selfie2anime/trainB'\nreal_dir = '/kaggle/input/selfie2anime/trainA'\n\n# Train the model\ntrain_style_transfer(anime_dir, real_dir)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:54:44.117259Z","iopub.execute_input":"2024-10-30T06:54:44.117677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nconverted_image = convert_to_anime('/kaggle/input/selfie2anime/testA/female_10328.jpg', '/kaggle/working/checkpoint_epoch_30.pth.tar')\nconverted_image.save('anime_style_output.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:51:29.005412Z","iopub.execute_input":"2024-10-30T06:51:29.006234Z","iopub.status.idle":"2024-10-30T06:51:29.244592Z","shell.execute_reply.started":"2024-10-30T06:51:29.006193Z","shell.execute_reply":"2024-10-30T06:51:29.243599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}